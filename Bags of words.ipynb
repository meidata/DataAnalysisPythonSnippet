{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "We want to eventually train a machine learning algorithm that predicts the number of upvotes a headline would receive. To do this, we'll need to be able to convert each headline to a numerical representation.\n",
    "\n",
    "One way to do this is with something called a bag of words model. The bag of words model represents each piece of text as a numerical vector.\n",
    "\n",
    "We'll look in more depth at each step in the bag of words process throughout this mission. Here's a high-level diagram showing how two sentences, I rode my horse to Berlin., and You rode my horse to Berlin in the winter. turn into a bag of words:\n",
    "\n",
    "1\n",
    "I\n",
    "r\n",
    "o\n",
    "d\n",
    "e\n",
    "m\n",
    "y\n",
    "h\n",
    "o\n",
    "r\n",
    "s\n",
    "e\n",
    "t\n",
    "o\n",
    "B\n",
    "e\n",
    "r\n",
    "l\n",
    "i\n",
    "n\n",
    ".\n",
    "2\n",
    "Y\n",
    "o\n",
    "u\n",
    "r\n",
    "o\n",
    "d\n",
    "e\n",
    "m\n",
    "y\n",
    "h\n",
    "o\n",
    "r\n",
    "s\n",
    "e\n",
    "t\n",
    "o\n",
    "B\n",
    "e\n",
    "r\n",
    "l\n",
    "i\n",
    "n\n",
    "i\n",
    "n\n",
    "t\n",
    "h\n",
    "e\n",
    "w\n",
    "i\n",
    "n\n",
    "t\n",
    "e\n",
    "r\n",
    ".\n",
    "B\n",
    "a\n",
    "g\n",
    "o\n",
    "f\n",
    "w\n",
    "o\n",
    "r\n",
    "d\n",
    "s\n",
    "i\n",
    "y\n",
    "o\n",
    "u\n",
    "r\n",
    "o\n",
    "d\n",
    "e\n",
    "m\n",
    "y\n",
    "h\n",
    "o\n",
    "r\n",
    "s\n",
    "e\n",
    "t\n",
    "o\n",
    "b\n",
    "e\n",
    "r\n",
    "l\n",
    "i\n",
    "n\n",
    "i\n",
    "n\n",
    "t\n",
    "h\n",
    "e\n",
    "w\n",
    "i\n",
    "n\n",
    "t\n",
    "e\n",
    "r\n",
    "1\n",
    "1\n",
    "0\n",
    "1\n",
    "1\n",
    "1\n",
    "1\n",
    "1\n",
    "0\n",
    "0\n",
    "0\n",
    "2\n",
    "0\n",
    "1\n",
    "1\n",
    "1\n",
    "1\n",
    "1\n",
    "1\n",
    "1\n",
    "1\n",
    "1\n",
    "The first step in creating a bag of words model is known as tokenization. In tokenization, we break a sentence into disconnected words.\n",
    "\n",
    "Here's a diagram, where we tokenize the two sentences from earlier:\n",
    "\n",
    "1\n",
    "I\n",
    "r\n",
    "o\n",
    "d\n",
    "e\n",
    "m\n",
    "y\n",
    "h\n",
    "o\n",
    "r\n",
    "s\n",
    "e\n",
    "t\n",
    "o\n",
    "B\n",
    "e\n",
    "r\n",
    "l\n",
    "i\n",
    "n\n",
    ".\n",
    "2\n",
    "Y\n",
    "o\n",
    "u\n",
    "r\n",
    "o\n",
    "d\n",
    "e\n",
    "m\n",
    "y\n",
    "h\n",
    "o\n",
    "r\n",
    "s\n",
    "e\n",
    "t\n",
    "o\n",
    "B\n",
    "e\n",
    "r\n",
    "l\n",
    "i\n",
    "n\n",
    "i\n",
    "n\n",
    "t\n",
    "h\n",
    "e\n",
    "w\n",
    "i\n",
    "n\n",
    "t\n",
    "e\n",
    "r\n",
    ".\n",
    "T\n",
    "o\n",
    "k\n",
    "e\n",
    "n\n",
    "i\n",
    "z\n",
    "a\n",
    "t\n",
    "i\n",
    "o\n",
    "n\n",
    "1\n",
    "[\n",
    "I\n",
    ",\n",
    "r\n",
    "o\n",
    "d\n",
    "e\n",
    ",\n",
    "m\n",
    "y\n",
    ",\n",
    "h\n",
    "o\n",
    "r\n",
    "s\n",
    "e\n",
    ",\n",
    "t\n",
    "o\n",
    ",\n",
    "B\n",
    "e\n",
    "r\n",
    "l\n",
    "i\n",
    "n\n",
    ".\n",
    "]\n",
    "2\n",
    "[\n",
    "Y\n",
    "o\n",
    "u\n",
    ",\n",
    "r\n",
    "o\n",
    "d\n",
    "e\n",
    ",\n",
    "m\n",
    "y\n",
    ",\n",
    "h\n",
    "o\n",
    "r\n",
    "s\n",
    "e\n",
    ",\n",
    "t\n",
    "o\n",
    ",\n",
    "B\n",
    "e\n",
    "r\n",
    "l\n",
    "i\n",
    "n\n",
    ",\n",
    "i\n",
    "n\n",
    ",\n",
    "t\n",
    "h\n",
    "e\n",
    ",\n",
    "w\n",
    "i\n",
    "n\n",
    "t\n",
    "e\n",
    "r\n",
    ".\n",
    "]\n",
    "As you can see, all we're doing is splitting each sentence into a list of tokens. The split happens when a space character is found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenized_headlines = []\n",
    "\n",
    "tokenized_headlines = [ line.split(\" \") for line in submissions['headline']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing\n",
    "We now have tokens, but they need some processing to make our predictions more accurate. We know that Berlin, Berlin., and berlin are all referring to the same word, but the computer doesn't know that unless we convert them all to be the same.\n",
    "\n",
    "We can do this by lowercasing, so Berlin is turned into berlin, and removing punctuation, so Berlin. becomes Berlin.\n",
    "\n",
    "1\n",
    "[\n",
    "I\n",
    ",\n",
    "r\n",
    "o\n",
    "d\n",
    "e\n",
    ",\n",
    "m\n",
    "y\n",
    ",\n",
    "h\n",
    "o\n",
    "r\n",
    "s\n",
    "e\n",
    ",\n",
    "t\n",
    "o\n",
    ",\n",
    "B\n",
    "e\n",
    "r\n",
    "l\n",
    "i\n",
    "n\n",
    ".\n",
    "]\n",
    "2\n",
    "[\n",
    "Y\n",
    "o\n",
    "u\n",
    ",\n",
    "r\n",
    "o\n",
    "d\n",
    "e\n",
    ",\n",
    "m\n",
    "y\n",
    ",\n",
    "h\n",
    "o\n",
    "r\n",
    "s\n",
    "e\n",
    ",\n",
    "t\n",
    "o\n",
    ",\n",
    "B\n",
    "e\n",
    "r\n",
    "l\n",
    "i\n",
    "n\n",
    ",\n",
    "i\n",
    "n\n",
    ",\n",
    "t\n",
    "h\n",
    "e\n",
    ",\n",
    "w\n",
    "i\n",
    "n\n",
    "t\n",
    "e\n",
    "r\n",
    ".\n",
    "]\n",
    "P\n",
    "r\n",
    "e\n",
    "p\n",
    "r\n",
    "o\n",
    "c\n",
    "e\n",
    "s\n",
    "s\n",
    "i\n",
    "n\n",
    "g\n",
    "1\n",
    "[\n",
    "i\n",
    ",\n",
    "r\n",
    "o\n",
    "d\n",
    "e\n",
    ",\n",
    "m\n",
    "y\n",
    ",\n",
    "h\n",
    "o\n",
    "r\n",
    "s\n",
    "e\n",
    ",\n",
    "t\n",
    "o\n",
    ",\n",
    "b\n",
    "e\n",
    "r\n",
    "l\n",
    "i\n",
    "n\n",
    "]\n",
    "2\n",
    "[\n",
    "y\n",
    "o\n",
    "u\n",
    ",\n",
    "r\n",
    "o\n",
    "d\n",
    "e\n",
    ",\n",
    "m\n",
    "y\n",
    ",\n",
    "h\n",
    "o\n",
    "r\n",
    "s\n",
    "e\n",
    ",\n",
    "t\n",
    "o\n",
    ",\n",
    "b\n",
    "e\n",
    "r\n",
    "l\n",
    "i\n",
    "n\n",
    ",\n",
    "i\n",
    "n\n",
    ",\n",
    "t\n",
    "h\n",
    "e\n",
    ",\n",
    "w\n",
    "i\n",
    "n\n",
    "t\n",
    "e\n",
    "r\n",
    "]\n",
    "Preprocessing doesn't have to be perfect, but the more we can help the computer group the same word together, the higher our prediction accuracy will be. It's useful to look through your tokens, and see if there are any instances of the same word that you haven't grouped together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "unique_tokens = []\n",
    "single_tokens = []\n",
    "\n",
    "\n",
    "compelete_token = [word for headLine in clean_tokenized for word in headLine]\n",
    "\n",
    "freqs = {}\n",
    "for word in compelete_token:\n",
    "    freqs[word] = freqs.get(word, 0) + 1\n",
    "    \n",
    "unique_tokens = []\n",
    "for key, value in freqs.items():\n",
    "    if value >1 :\n",
    "        unique_tokens.append(key)\n",
    "        \n",
    "counts = pd.DataFrame(0, index=np.arange(len(clean_tokenized)), columns=unique_tokens)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
